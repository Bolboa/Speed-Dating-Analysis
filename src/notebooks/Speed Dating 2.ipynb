{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Dating Analysis Part 2\n",
    "\n",
    "Here we will talk about the learning algorithms used to measure the accuracy of the models implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import *scikit learn* and load the data set. The data set we are loading is the preprocessed version of the data set which we saved as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn import datasets, svm, model_selection, tree, metrics, preprocessing\n",
    "\n",
    "# Read dataframe.\n",
    "data = pd.read_pickle('../dating')\n",
    "\n",
    "# Set target column.\n",
    "target = \"match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "The first learning model that we will apply is a simple Decision Tree. The way it works is that it splits the data recursively. Every feature will be split based on its possible outcomes and the entropy between each new subset and the target feature *match* is calculated. We measure the information gain of every new subset by substracting the entropy of the dataset before the split by the sum entropy of each new subset. The node with the most information gain becomes the root node of our tree. The data will now be split based on this root node. In the next iteration, this root node is not in the data set and now we repeat the same entropy calculation minus one feature. This is done until all paths in the tree reaches leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score (No Cross-validation): 0.896181384248\n"
     ]
    }
   ],
   "source": [
    "# Drop the target label, which we save separately.\n",
    "X = data.drop([target], axis=1).values\n",
    "y = data[target].values\n",
    "\n",
    "# Split data in a 20/80 split.\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize a decison tree with a max_depth of 10.\n",
    "clf_tree = tree.DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# Fit the model with the training data.\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Use the test data to calculate the score.\n",
    "print(\"Decision Tree Score (No Cross-validation):\", clf_tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy rating is pretty good, but in order for the output to be more meaningful, it is necessary to apply K-fold Cross Validation to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way K-fold Cross Validation works is that, in our instance, it splits the data set 80/20 as training data and testing data. But we find every permutation of test data and train data. So the test data will be unique on every validation. We then calculate the average of the total accuracy rate from every permutation that was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_permutations_cross_val(X, y, model):\n",
    "\n",
    "    # Split data 20/80 to be used in a K-Fold Cross Validation with unique permutations.\n",
    "    shuffle_validator = model_selection.ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Calculate the score of the model after Cross Validation has been applied to it. \n",
    "    scores = model_selection.cross_val_score(model, X, y, cv=shuffle_validator)\n",
    "\n",
    "    # Print out the score (mean), as well as the variance.\n",
    "    print(\"Accuracy: %0.4f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function for our Decision Tree and run it with K-fold Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score (No Cross-validation): 0.897374701671\n",
      "Accuracy: 0.9056 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "def decision_tree(target, data):\n",
    "\n",
    "    # Drop the target label, which we save separately.\n",
    "    X = data.drop([target], axis=1).values\n",
    "    y = data[target].values\n",
    "\n",
    "    # Split data in a 20/80 split.\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Initialize a decison tree with a max_depth of 10.\n",
    "    clf_tree = tree.DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\n",
    "    # Fit the model with the training data.\n",
    "    clf_tree.fit(X_train, y_train)\n",
    "\n",
    "    # Use the test data to calculate the score.\n",
    "    print(\"Decision Tree Score (No Cross-validation):\", clf_tree.score(X_test, y_test))\n",
    "\n",
    "    # Implement the decision tree again using Cross Validation.\n",
    "    unique_permutations_cross_val(X, y, clf_tree)\n",
    "\n",
    "    return clf_tree\n",
    "\n",
    "decision_tree = decision_tree(target, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "The next learning model we will implement is the Random Forest algorithm. The problem with Decision Trees is that they overfit the data way too strongly. The Random Forest algorithm is intended to add some randomness to the trained data. The basic idea of a Random Forest is that it takes a bunch of weak learners and combines them to create a strong learner. So in essence, the Random Forest algorithm creates a bunch of Decision Tree with some random value selected from the data set. The accuracy rating of every prediction is averaged over the accuracy prediction of all Decision Trees. When implementing the Random Forest algorithm, we will also implement K-fold Cross Validation alongside it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9203 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "def random_forest(target, data):\n",
    "\n",
    "    # Drop the target label, which we save separately.\n",
    "    X = data.drop([target], axis=1).values\n",
    "    y = data[target].values\n",
    "\n",
    "    # Random Forest Classifier.\n",
    "    clf_tree = ske.RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "    \n",
    "    # Implement the decision tree again using Cross Validation.\n",
    "    unique_permutations_cross_val(X, y, clf_tree)\n",
    "\n",
    "    clf_tree.fit(X, y)\n",
    "    \n",
    "    return clf_tree\n",
    "\n",
    "rnd_forest = random_forest(target, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "The next model we will implement is Gradient Boosting. The idea behind a Gradient Boosting algorithm is that we take a weak learner and make it stronger by changing the weight distributions. So first the Decision Tree is trained and the loss is calculated using some loss function. Based on the results of the loss function, the weights of the Decision Tree are altered depending on how wrong it is, we want the Mean Squared Error to be as small as possible between the prediction and the actual value. The weights for the predictions that are far off, and they are decreased for those that are too close. This will eventually result in the total Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9236 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "def gradient_boosting(target, data):\n",
    "\n",
    "    # Drop the target label, which we save separately.\n",
    "    X = data.drop([target], axis=1).values\n",
    "    y = data[target].values\n",
    "\n",
    "    # Gradient Boosting.\n",
    "    clf_gradient = ske.GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "\n",
    "    # Implement the decision tree again using Cross Validation.\n",
    "    unique_permutations_cross_val(X, y, clf_gradient)\n",
    "\n",
    "    clf_gradient.fit(X, y)\n",
    "    \n",
    "    return clf_gradient\n",
    "\n",
    "grd_boost = gradient_boosting(target, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our model are now constructed and it seems that Gradient Boosting performs best out of all the models. However, it would also be neat if we could create a fake participant in the data set to see what this participant's particular match rate would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shar4_1': 9.2024063356685968, 'intel1_1': 17.698318214370971, 'fun': 5.5750527229872224, 'dec_o': 0.37168775364048701, 'race': 2.3728813559322033, 'sports': 5.6056084468720204, 'pf_o_fun': 15.24855887310378, 'shar1_1': 10.233034986820034, 'pid': 244.7923288325965, 'exphappy': 4.8323360459550022, 'music': 6.837192647409883, 'shar1_2': 10.888342126088105, 'samerace': 0.36070661255669612, 'attr2_1': 27.137091191215085, 'wave': 9.7849128670327055, 'sinc': 6.243430746561887, 'goal': 1.8694199092862258, 'attr': 5.3761150377772369, 'prob_o': 4.5220987654320979, 'amb2_1': 10.096976605395083, 'exercise': 5.492957746478873, 'tvsports': 4.0216042014800673, 'id': 7.8634355974692607, 'like': 5.331541569731848, 'intel1_2': 15.311486415193883, 'idg': 15.204941513487706, 'hiking': 4.9541656719980907, 'met': 0.90187554341075638, 'attr1_1': 19.71357885926103, 'fun1_1': 15.30945094294581, 'field_cd': 6.6911113769589665, 'pf_o_amb': 9.2083429742106535, 'amb3_2': 6.4692693220786071, 'pf_o_att': 19.732108491700743, 'career_c': 4.6604201480066845, 'clubbing': 4.9915254237288131, 'imprace': 3.4090475053712104, 'theater': 5.9345905944139412, 'position': 7.9853186918118881, 'attr1_2': 22.566331125827816, 'shopping': 4.981976605395082, 'like_o': 5.34424561833558, 'pf_o_int': 17.742766418089971, 'museums': 6.0596801145858201, 'fun2_1': 16.071728336118404, 'amb_o': 5.8597600309637459, 'sinc_o': 6.2211633054599123, 'length': 1.6079545454545454, 'amb': 5.8881587424300994, 'go_out': 1.8769396037240391, 'amb3_1': 6.6285919540229887, 'art': 5.8330150393888758, 'attr3_1': 6.2351532567049812, 'intel': 6.4300936191180096, 'pf_o_sin': 15.119748616790956, 'sinc1_1': 15.008081061692971, 'sinc3_2': 6.838169348456872, 'sinc3_1': 7.2446120689655169, 'concerts': 5.9318453091429939, 'int_corr': 0.16747566983058995, 'tv': 4.6939603724039154, 'partner': 7.8734781570780612, 'sinc2_1': 11.369745762711863, 'shar': 4.6968456375838921, 'intel_o': 6.3937469166255552, 'fun1_2': 15.352804009496177, 'fun3_1': 6.7529932950191567, 'dining': 6.784793506803533, 'yoga': 3.76056338028169, 'pf_o_sha': 10.269676289406933, 'attr3_2': 6.161171194935374, 'round': 14.826927667701122, 'age_o': 22.943996145971337, 'gaming': 3.391978992599666, 'shar2_1': 10.291694915254238, 'zipcode': 162.93196466937218, 'intel3_2': 7.0939066209443409, 'amb1_1': 9.4518893403441684, 'reading': 6.7086416805920264, 'movies': 6.9201480066841725, 'condtn': 1.613034137025543, 'intel3_1': 7.3503352490421454, 'from': 101.04678920983528, 'intel2_1': 12.348614227739317, 'age': 22.8947999041457, 'date': 4.3930622009569369, 'race_o': 2.4001200480192075, 'attr_o': 5.4123091952619369, 'numdat_2': 2.0160222457627119, 'met_o': 1.7001866832607342}\n"
     ]
    }
   ],
   "source": [
    "# Extract all column headers.\n",
    "header_list = list(data.columns.values)\n",
    "\n",
    "# Get the mean values for all columns and store them in a list.\n",
    "mean_values = [data[header].mean() for header in header_list]\n",
    "\n",
    "# Map each column header to its respective mean.\n",
    "header_dict = dict(zip(header_list, mean_values))\n",
    "\n",
    "# Remove the target header as this is not part of the dataset.\n",
    "del header_dict[target]\n",
    "\n",
    "print(header_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a dictionary of the data set where the *keys* are the column names and the *values* are the mean values of each column. The idea is to create an average participant and because it is stored in a dictionary, we can alter any feature to whatever we want and see if the prediction changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Extract only the mean values after values have been changed.\n",
    "extract_values = list(header_dict.values())\n",
    "\n",
    "# Random Forest can predict the match rate if you give a value for every column.\n",
    "# In this case the values represent the mean of every column.\n",
    "predictions = rnd_forest.predict([extract_values])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the random forest predicted a match rate of 0. We can alter some features in order to try and achieve a match rate of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Key = Column Header,\n",
    "# Value = Mean of Column,\n",
    "# Storing it in a dictionary makes it easier to change values so\n",
    "# as to see how the prediction changes.\n",
    "header_dict['exercise'] = 10\n",
    "header_dict['age'] = 23\n",
    "header_dict['movies'] = 10\n",
    "header_dict['reading'] = 10\n",
    "header_dict['music'] = 10\n",
    "header_dict['samerace'] = 1\n",
    "\n",
    "# Extract only the mean values after values have been changed.\n",
    "extract_values = list(header_dict.values())\n",
    "\n",
    "# Random Forest can predict the match rate if you give a value for every column.\n",
    "# In this case the values represent the mean of every column.\n",
    "predictions = rnd_forest.predict([extract_values])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made an ideal candidate. He likes movies, reading, music, and exercising. He is also of the same race as his partner, and this gives him a match rate of 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
